{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "-lZnStGTfgex",
      "metadata": {
        "id": "-lZnStGTfgex"
      },
      "source": [
        "We're going to look into the Runnable - one of the core LangChain primitives, and how to use LCEL (LangChain Expressive Language)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cuzkefx76Zyb",
      "metadata": {
        "id": "Cuzkefx76Zyb"
      },
      "source": [
        "# LangChain Runnables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22b1981b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain-core)\n",
            "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain-core)\n",
            "  Downloading langsmith-0.3.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/jeffzo/projects/Generative-AI-on-Google-Cloud-with-LangChain/.venv/lib/python3.11/site-packages (from langchain-core) (24.2)\n",
            "Collecting pydantic<3.0.0,>=2.5.2 (from langchain-core)\n",
            "  Using cached pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core)\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/jeffzo/projects/Generative-AI-on-Google-Cloud-with-LangChain/.venv/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading orjson-3.10.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
            "Collecting requests<3,>=2 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.5.2->langchain-core)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.5.2->langchain-core)\n",
            "  Using cached pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Using cached charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.3.1-py3-none-any.whl (332 kB)\n",
            "Using cached pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
            "Using cached pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
            "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
            "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (633 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.7/633.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl (194 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: zstandard, urllib3, tenacity, sniffio, PyYAML, pydantic-core, orjson, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, requests, pydantic, jsonpatch, httpcore, anyio, requests-toolbelt, httpx, langsmith, langchain-core\n",
            "Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.8.0 certifi-2024.12.14 charset-normalizer-3.4.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.31 langsmith-0.3.1 orjson-3.10.15 pydantic-2.10.5 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 urllib3-2.3.0 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b2sJ75wlbmcPD1iXfJXgJYdO",
      "metadata": {
        "id": "b2sJ75wlbmcPD1iXfJXgJYdO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wdCkf4Vz6mHk",
      "metadata": {
        "id": "wdCkf4Vz6mHk"
      },
      "source": [
        "Let's start with a very simple *Runnable* created from a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "yggCCEpcMkYu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1728719813457,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "yggCCEpcMkYu",
        "outputId": "b57b4345-644f-48c3-b050-219139f1fe99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "runnable = RunnableLambda(lambda x: x + 1)\n",
        "\n",
        "runnable.invoke(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SNIRyH7W6uh7",
      "metadata": {
        "id": "SNIRyH7W6uh7"
      },
      "source": [
        "Now we can put together our first chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "j-YgLSRxP79h",
      "metadata": {
        "id": "j-YgLSRxP79h"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from langchain_core.runnables import Runnable, RunnableConfig\n",
        "\n",
        "def increment_by_one(x: int) -> int:\n",
        "  return x + 1\n",
        "\n",
        "def fake_llm(x: int) -> str:\n",
        "  return f\"Result = {x}\"\n",
        "\n",
        "\n",
        "class MyFirstChain(Runnable[int, str]):\n",
        "\n",
        "   def invoke(self, input: str, config: Optional[RunnableConfig] = None) -> str:\n",
        "    increment = increment_by_one(input)\n",
        "    return fake_llm(increment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "y-ndl3NV629R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1728719655949,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "y-ndl3NV629R",
        "outputId": "23a2f5e9-7994-4e59-a065-cc67c3618c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result = 2\n"
          ]
        }
      ],
      "source": [
        "chain = MyFirstChain()\n",
        "result = chain.invoke(1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JqjXgdKE64r1",
      "metadata": {
        "id": "JqjXgdKE64r1"
      },
      "source": [
        "And we can do the same much easier with LangChain Expressive Language (LCEL):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3SxAN_7tP8AE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 407,
          "status": "ok",
          "timestamp": 1728719658625,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "3SxAN_7tP8AE",
        "outputId": "11799cb4-4cd7-4b9d-bc5e-d53c244d3cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result = 2\n"
          ]
        }
      ],
      "source": [
        "chain = (\n",
        "    RunnableLambda(increment_by_one)\n",
        "    | RunnableLambda(fake_llm)\n",
        ")\n",
        "\n",
        "\n",
        "result = chain.invoke(1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "koBFjlmFoo-d",
      "metadata": {
        "id": "koBFjlmFoo-d"
      },
      "source": [
        "Actually, you should only convert the last element explicitly to RunnableLambda, and LangChain would take care about else for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4RQD7un3omQw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 386,
          "status": "ok",
          "timestamp": 1728719669737,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "4RQD7un3omQw",
        "outputId": "b4ab70a1-3a27-4d41-b902-5fb0fc416c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result = 2\n"
          ]
        }
      ],
      "source": [
        "chain = (\n",
        "    increment_by_one\n",
        "    | RunnableLambda(fake_llm)\n",
        ")\n",
        "\n",
        "\n",
        "result = chain.invoke(1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ktKvZaWSDcDf",
      "metadata": {
        "id": "ktKvZaWSDcDf"
      },
      "source": [
        "LCEL with | operator is actually equivalent to creating a RunnableSequence explicitly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7Il9hIG67KWS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1728719675053,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "7Il9hIG67KWS",
        "outputId": "07d0cb4b-a359-4023-d1be-bbedef50678d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "\n",
        "a = (RunnableLambda(increment_by_one) | RunnableLambda(fake_llm))\n",
        "b = RunnableSequence(RunnableLambda(increment_by_one), RunnableLambda(fake_llm))\n",
        "\n",
        "a == b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nB9pk7zvDjlX",
      "metadata": {
        "id": "nB9pk7zvDjlX"
      },
      "source": [
        "## RunnableParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HNaDfKslDnQU",
      "metadata": {
        "id": "HNaDfKslDnQU"
      },
      "source": [
        "Another powerful LCEL primitive is RunnableParallel. You pass multiple chains as named arguments, and it runs them in parallel and combines their outputs into a dict with keys being argnames and values being outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ZMjHbuPEGzFv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1728719701493,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "ZMjHbuPEGzFv",
        "outputId": "36f3757b-5fc0-4a1c-830f-1a1a452a05d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'step1': 'Result = 2', 'step2': 'Result = 1'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain = RunnableParallel(step1=increment_by_one | RunnableLambda(fake_llm), step2=fake_llm)\n",
        "\n",
        "chain.invoke(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tdPrd0SFk2J3",
      "metadata": {
        "id": "tdPrd0SFk2J3"
      },
      "source": [
        "You can also easily compose chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "twY0aP3Wk1rL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 385,
          "status": "ok",
          "timestamp": 1728719704608,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "twY0aP3Wk1rL",
        "outputId": "c91b8a7d-7c8c-4b66-afdb-ae1add6a9877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'step1': 'Result = 3', 'step2': 'Result = 2'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain1 = increment_by_one | chain\n",
        "chain1.invoke(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6muSPEW5j-VC",
      "metadata": {
        "id": "6muSPEW5j-VC"
      },
      "source": [
        "You don't need to use the RunnableParallel constructor, you can just combine the chains within a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "H_MedXfH7KYe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 325,
          "status": "ok",
          "timestamp": 1728719713551,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "H_MedXfH7KYe",
        "outputId": "1cb72e73-d001-441f-92fb-f387b3c3f4b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'step1': 'Result = 3', 'step2': 'Result = 2'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain2 = ( RunnableLambda(increment_by_one)\n",
        "  | {\"step1\": increment_by_one | RunnableLambda(fake_llm), \"step2\": fake_llm}\n",
        ")\n",
        "\n",
        "chain2.invoke(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "_4ZszelGjbXm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1728719713551,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "_4ZszelGjbXm",
        "outputId": "1a759e0c-4fce-4f38-b09d-fdddca0d2b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(chain1 == chain2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P3U9jy49Iq_n",
      "metadata": {
        "id": "P3U9jy49Iq_n"
      },
      "source": [
        "## itemgetter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aShrcfUOwWFU",
      "metadata": {
        "id": "aShrcfUOwWFU"
      },
      "source": [
        "We typically pass input as dictionaries, and there's a convinient way to retrieve an element from a dictionary with a built-in *itemgetter* function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "qBznqy-UIr_o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 365,
          "status": "ok",
          "timestamp": 1728719724085,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "qBznqy-UIr_o",
        "outputId": "07d00983-4291-437b-b22a-fc08e08d3042"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Result = 2'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "chain = (\n",
        "  itemgetter(\"x\")\n",
        "  | RunnableLambda(increment_by_one)\n",
        "  | fake_llm\n",
        ")\n",
        "\n",
        "\n",
        "chain.invoke({\"x\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J9RZqvhkp3Tb",
      "metadata": {
        "id": "J9RZqvhkp3Tb"
      },
      "source": [
        "## RunnablePassThrough"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bmPFH3d8whPv",
      "metadata": {
        "id": "bmPFH3d8whPv"
      },
      "source": [
        "And we can modify dictionaries in-place with Runnables (by assigning values into a dictionary or create new dictionaries). That's how we produce an output dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "IKhu5TxFJvkh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1728719725684,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "IKhu5TxFJvkh",
        "outputId": "568122f6-0769-4d59-e5df-ede68cb3686e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'origin': 1, 'output': 2}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain_rps = RunnableParallel(\n",
        "    origin=RunnablePassthrough(),\n",
        "    output=increment_by_one\n",
        ")\n",
        "\n",
        "chain_rps.invoke(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rweY_Q_eHkBT",
      "metadata": {
        "id": "rweY_Q_eHkBT"
      },
      "source": [
        "Now let's create a new dictionary by adding (assigning) additional values to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "_DftSnSuI177",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1728726318723,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "_DftSnSuI177",
        "outputId": "a8359b56-7e7d-4fa3-aedc-b8fb7954a278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'x': 1, 'y': 2}\n"
          ]
        }
      ],
      "source": [
        "chain_assign = RunnablePassthrough().assign(y=itemgetter(\"x\") | RunnableLambda(increment_by_one))\n",
        "\n",
        "query = {\"x\": 1}\n",
        "result = chain_assign.invoke({\"x\": 1})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p7bUB4MOHrD-",
      "metadata": {
        "id": "p7bUB4MOHrD-"
      },
      "source": [
        "And let's put it all together by creating a more complex dictionary (with sub-chains):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "m43_mgUFp-Gi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 346,
          "status": "ok",
          "timestamp": 1728726368155,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "m43_mgUFp-Gi",
        "outputId": "4e8de85c-da26-4753-c171-d609504dfd2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'origin': {'x': 1, 'length': 2}, 'modified': 2}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_rps = RunnableParallel(\n",
        "    origin=RunnablePassthrough().assign(length=lambda input: input[\"x\"]+1),\n",
        "    modified=lambda input: increment_by_one(input[\"x\"])\n",
        ")\n",
        "\n",
        "chain_rps.invoke({\"x\": 1})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Chapter 1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
