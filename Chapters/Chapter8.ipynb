{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeaeXdqkFCZD"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd3D45ERFGtH"
      },
      "source": [
        "Let's start by building a basic chatbot.For conversation history, we require a CloudSQL instance running in a Google\n",
        "Cloud project that the user or service account that executes your LangChain\n",
        "code is authenticated with. Look at the Google Cloud documentation for more\n",
        "information about [setting up and connecting to CloudSQL instances](https://cloud.google.com/sql/docs/mysql/connect-instance-cloud-shell).\n",
        "\n",
        "These libraries contain the core components of LangChain, as well as the ability to\n",
        "handle memory in a local MySQL database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU ipykernel\n",
        "!pip install -U pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUqtoWT8wuyh",
        "outputId": "bcabe299-58f2-455a-da35-88b5662d903e"
      },
      "outputs": [],
      "source": [
        "# Base components for using LangChain on Google Cloud\n",
        "! pip install -qU langchain-google-vertexai\n",
        "# LangGraph enables creation of graph-based chat agents\n",
        "! pip install -qU langgraph httpx\n",
        "# Grandalf is a tool to visualize graphs (like LangGraph)\n",
        "! pip install -qU grandalf # for visualizing the graph\n",
        "# Enables LangGraph to use SQLite for memory handling\n",
        "! pip install -qU langgraph-checkpoint-sqlite # for chatbot memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EWukDcrbSRaA"
      },
      "outputs": [],
      "source": [
        "# If you are using Google Colab, you need to restart your kernel after installation\n",
        "# You can skip this step in Vertex AI Workbench\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hITWjJcqSa9_"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook on Google Colab, run the cell below to authenticate your environment\n",
        "# If you are not using Google Colab, ensure that your Jupyter Notebook instance is connected to Google Cloud\n",
        "# See https://cloud.google.com/sdk/gcloud for more information\n",
        "\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tt8qxqnDFgbc"
      },
      "outputs": [],
      "source": [
        "# Set your GCP project and region\n",
        "\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"genai-app-bldr\"  # @param {type:\"string\"}\n",
        "REGION = \"asia-northeast3\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yFFgUiojF-TI"
      },
      "outputs": [],
      "source": [
        "# Import the class to interact with a Google Cloud Vertex AI Chatbot and set up a gemini-flash model\n",
        "\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "model = ChatVertexAI(model=\"gemini-2.0-flash\", location=\"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw6ZZFonUw47"
      },
      "source": [
        "Let's start with a simple conversation. Conversations are always an exchange of `HumanMessage` and `AIMessage`. The `HumanMessage` is provided by the user, while the `AIMessage` is generated by the model. When you print the full `AIMessage`, you will see that it comes with a lot of metadata, such as safety and usage information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-D8aDybQeTH",
        "outputId": "06a8f1b6-2cd4-4b6f-fb71-8f5e925e0e53"
      },
      "outputs": [],
      "source": [
        "# First, we send a HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model.invoke([HumanMessage(content=\"Hello, my name is Max!\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjl7byrhTTua",
        "outputId": "b113b33f-2cb5-4a0a-cefc-1ab1ad24c11f"
      },
      "outputs": [],
      "source": [
        "# Unfortunately, our chatbot does not have memory, and will not remember previous messages\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model.invoke([HumanMessage(content=\"What is my name?\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eTf49MHUUFB",
        "outputId": "27b09522-d76f-4f66-f059-cceb9d80c255"
      },
      "outputs": [],
      "source": [
        "# To work around this, we can manually send the conversation history to the bot\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "model.invoke([\n",
        "    HumanMessage(content=\"Hello, my name is Max!\"),\n",
        "    AIMessage(content=\"Hello Max, it's a pleasure to meet you. How can I assist you today?\"),\n",
        "    HumanMessage(\"What is my name?\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAcLRtktVO_B"
      },
      "source": [
        "Sending the full message conversation on every invocation is cumbersome and requires a lot of coding work on your part. Luckily, Langchain have released Langgraph, which handles memory using the checkpoint class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lANkoBMUroC",
        "outputId": "81336257-9a76-43d9-f505-2840ec944621"
      },
      "outputs": [],
      "source": [
        "# Import necessary langgraph classes to define the chatbot and handle memory for you\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Create the agent\n",
        "\n",
        "class State(TypedDict):\n",
        "    # update add_messages to append messages not overwrite\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "class StateMachine:\n",
        "\n",
        "    def __init__(self, memory):\n",
        "\n",
        "        model = ChatVertexAI(model=\"gemini-1.5-pro\")\n",
        "\n",
        "        # Define a system prompt that defines how the agent should interact with users\n",
        "        system_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"system\",\n",
        "                \"\"\"You are an expert advisor in a bank. Your name is Terry.\n",
        "                You advise users on the three loan products of the bank:\n",
        "                1/ Short term loan, 6 month duration, 5% interest rate, up to USD 10000. Great for making quick payments.\n",
        "                2/ Mid-term loan, 24 month duration, 3% interest rate, up to USD 20000. Great for buying used cars.\n",
        "                3/ Long-term loan, 60 month duration, 2% interest rate, up to USD 50000. Great for buying a new car or a cheap house.\n",
        "                Only respond to user questions on these loans.\n",
        "                    \"\"\"\n",
        "            ),\n",
        "            (\"placeholder\", \"{messages}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "        )\n",
        "\n",
        "        # Bind the system_prompt to the model\n",
        "        sally = system_prompt | model\n",
        "\n",
        "        def chatbot(state: State):\n",
        "            return {\"messages\": [sally.invoke(state[\"messages\"])]}\\\n",
        "\n",
        "        graph_builder = StateGraph(State)\n",
        "\n",
        "        graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "        # Set a finish point. This instructs the graph \"any time this node is run, you can exit.\"\n",
        "        graph_builder.set_finish_point(\"chatbot\")\n",
        "        graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "        # If you use memory, you need to provide a thread id to the chatbot\n",
        "        self.thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "        self.chain = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "    def respond(self, USER_MESSAGE:str):\n",
        "        result = self.chain.invoke({\"messages\": (\"user\", USER_MESSAGE)}, self.thread)\n",
        "\n",
        "        # Need to cut off the first 80 characters, which say \"AI Message\"\n",
        "        return result[\"messages\"][-1].pretty_repr()[80:]\n",
        "\n",
        "\n",
        "\n",
        "# Create memory for the bot\n",
        "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
        "  # Initiate the memory\n",
        "  chatbot = StateMachine(memory)\n",
        "\n",
        "  # Draw the graph\n",
        "  chatbot.chain.get_graph().print_ascii()\n",
        "\n",
        "  print(\"Agent will keep going until you say 'STOP'\")\n",
        "  question = \"\"\n",
        "  while \"STOP\" not in question:\n",
        "      question = input(\"User: \")\n",
        "      answer = chatbot.respond(question)\n",
        "      print(\"Bot: \" + answer)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
