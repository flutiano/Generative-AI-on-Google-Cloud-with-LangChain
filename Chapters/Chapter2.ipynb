{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "zu0Iuy3KO0cA",
      "metadata": {
        "id": "zu0Iuy3KO0cA"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BaJLWi0dO3eC",
      "metadata": {
        "id": "BaJLWi0dO3eC"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install --upgrade langchain-core\n",
        "pip install --upgrade langchain_google_vertexai[anthropic,all]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-_CJH3GmPBQ7",
      "metadata": {
        "id": "-_CJH3GmPBQ7"
      },
      "source": [
        "## Model invocation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BN0CMdEnO6Zz",
      "metadata": {
        "id": "BN0CMdEnO6Zz"
      },
      "source": [
        "Let's invoke a default model:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b9ac03",
      "metadata": {},
      "source": [
        "Doesn't work any longer. The default model, text-bison, was deprecated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rcHLZQKyIBuf36cY21y8ikfp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "executionInfo": {
          "elapsed": 4355,
          "status": "ok",
          "timestamp": 1717219187880,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "rcHLZQKyIBuf36cY21y8ikfp",
        "outputId": "e30728e7-c95f-4f4d-d589-c52c8a5d9dfe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "llm = VertexAI()\n",
        "llm.invoke(\"Which question can you answer?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95VQauEfPWfC",
      "metadata": {
        "id": "95VQauEfPWfC"
      },
      "source": [
        "We can see, that the default version is `text-bison`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wVJurhW_3HuV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1717219204569,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "wVJurhW_3HuV",
        "outputId": "c17b4b58-544d-47ea-ac53-aab007855fc4"
      },
      "outputs": [],
      "source": [
        "print(llm.model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LKUg0tH1Pec4",
      "metadata": {
        "id": "LKUg0tH1Pec4"
      },
      "source": [
        "Now let's change the model name and use Gemini-pro-1.5 running in Europe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hram3qhT2Dgo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3120,
          "status": "ok",
          "timestamp": 1717219263589,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "hram3qhT2Dgo",
        "outputId": "18ab7b23-4191-4751-d1d8-aab39883cfbf"
      },
      "outputs": [],
      "source": [
        "llm_gemini = VertexAI(model_name=\"gemini-1.5-pro-001\", location=\"europe-west1\")\n",
        "print(llm_gemini.invoke(\"Which question can you answer?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g25vPO0PPn4g",
      "metadata": {
        "id": "g25vPO0PPn4g"
      },
      "source": [
        "Let's stream the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kltzGhOnEdGp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3787,
          "status": "ok",
          "timestamp": 1717219274962,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "kltzGhOnEdGp",
        "outputId": "65d5ef4f-d415-47ab-97f4-71643eca630a"
      },
      "outputs": [],
      "source": [
        "for chunk in llm_gemini.stream(\"Write a poem about Google Cloud and LangChain\"):\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gz3Ni9RuPw5d",
      "metadata": {
        "id": "Gz3Ni9RuPw5d"
      },
      "source": [
        "Now let's override the default safety settings, and also control the length of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAewe4qE2t5S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1323,
          "status": "ok",
          "timestamp": 1717219314090,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "DAewe4qE2t5S",
        "outputId": "078fdd3e-2e8b-4d4d-f386-ba59d0cb64f5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
        "\n",
        "\n",
        "for chunk in llm_gemini.stream(\"Write a poem about Google Cloud and LangChain\", \n",
        "                               temperature=0.9, \n",
        "                               max_output_tokens=200, \n",
        "#                               stop=[\".\"], \n",
        "                               safety_settings={HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE}\n",
        "                               ):\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iBXhuJc0P7yB",
      "metadata": {
        "id": "iBXhuJc0P7yB"
      },
      "source": [
        "## LangChain interfaces: PromptTemplate and Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9Gl2B2SWQBER",
      "metadata": {
        "id": "9Gl2B2SWQBER"
      },
      "source": [
        "Let's use a PromptTemplate and build our first chain (a sequence of steps we'd like to orchestrate):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PjhQTAPN4iSB",
      "metadata": {
        "executionInfo": {
          "elapsed": 368,
          "status": "ok",
          "timestamp": 1717219358987,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "PjhQTAPN4iSB"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Extract {entities} entities from the item description:\\n{description}\\n.\"\n",
        "    \"Answer with a valid json as an output.\"\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm_gemini | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O3qEmAYyQF5W",
      "metadata": {
        "id": "O3qEmAYyQF5W"
      },
      "source": [
        "A prompt template is a runnable that substitutes parameters into the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xyuKbocd6r9O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 313,
          "status": "ok",
          "timestamp": 1717219403609,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "xyuKbocd6r9O",
        "outputId": "8e55ae61-aea3-4ed2-ecfe-9a1a4c2107fa"
      },
      "outputs": [],
      "source": [
        "s = prompt_template.invoke({\"description\": \"A\", \"entities\": \"B\"})\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Odw7Mq9i8K63",
      "metadata": {
        "id": "Odw7Mq9i8K63"
      },
      "source": [
        "Let's take a description of a Pixel 7a phone from this [website](https://store.google.com/product/pixel_7a?hl=de) (a few first paragraphs) and pass it to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HYzSesA54iUT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1176,
          "status": "ok",
          "timestamp": 1717219422222,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "HYzSesA54iUT",
        "outputId": "b81953a5-c3df-4be6-89c8-4e5b3f8d487a"
      },
      "outputs": [],
      "source": [
        "description = \"\"\"Meet Google Pixel 7a, our latest A-Series phone that delivers all the helpfulness of Google for less. It’s built with Google Tensor G2, our flagship processor, and Titan M2, our dedicated security chip, making it faster, more efficient and more secure.\n",
        "\n",
        "Pixel 7a is packed with many of the must-have features of our premium phones that are now available on an A-series phone for the first time — like Face Unlock, 8GB of RAM, an up to 90Hz Smooth Display and wireless charging. Pixel 7a provides the core Pixel experience, starting at $499.\"\"\"\n",
        "\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GDySuJQ-8a3m",
      "metadata": {
        "id": "GDySuJQ-8a3m"
      },
      "source": [
        "As we can see, the model was able to parse the attributes we asked for, and the parser transformed it into a valid json object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sgmajz0M5-YB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 333,
          "status": "ok",
          "timestamp": 1717219437432,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "sgmajz0M5-YB",
        "outputId": "21331e53-dc4b-46db-dc8f-052a6f4a07a7"
      },
      "outputs": [],
      "source": [
        "type(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jn51umPkQTwv",
      "metadata": {
        "id": "jn51umPkQTwv"
      },
      "source": [
        "## Chat models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "zv_00BRl3-3r",
      "metadata": {
        "executionInfo": {
          "elapsed": 329,
          "status": "ok",
          "timestamp": 1717219450210,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "zv_00BRl3-3r"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import BaseMessage, HumanMessage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AzqBqHpLQWfH",
      "metadata": {
        "id": "AzqBqHpLQWfH"
      },
      "source": [
        "Now let's create our first message. In practice, we'll use classes that inherit from a BaseMessage (and a type, or role, is already defined):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "T_9YxBRh5Kcm",
      "metadata": {
        "executionInfo": {
          "elapsed": 311,
          "status": "ok",
          "timestamp": 1717219489665,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "T_9YxBRh5Kcm"
      },
      "outputs": [],
      "source": [
        "message = BaseMessage(content=\"Hi, how are you?\", type=\"human\", additional_kwargs={\"chapter\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZuX8NwtQ5bl1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1518,
          "status": "ok",
          "timestamp": 1717219515786,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "ZuX8NwtQ5bl1",
        "outputId": "c5dd6a24-b45c-45c3-a61e-ec5f0c86699f"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "chat_model = ChatVertexAI(model_name=\"gemini-1.5-pro-001\")\n",
        "message = HumanMessage(content=\"Hi, how are you?\")\n",
        "answer = chat_model.invoke([message])\n",
        "print(answer.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XAYdwI0h6AJP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 962,
          "status": "ok",
          "timestamp": 1717219521916,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "XAYdwI0h6AJP",
        "outputId": "6f9b98b2-b4e9-4d93-f326-ef1138216f97"
      },
      "outputs": [],
      "source": [
        "message2 = HumanMessage(content=\"Can you tell me how much is 2+2?\")\n",
        "answer2 = chat_model.invoke([message, answer, message2], temperature=0.9)\n",
        "print(answer2.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5kAOvDwR5UM-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 316,
          "status": "ok",
          "timestamp": 1717219523970,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "5kAOvDwR5UM-",
        "outputId": "323f2db4-8eb5-4fe6-a6fe-a93ff88b113a"
      },
      "outputs": [],
      "source": [
        "print(answer.response_metadata[\"usage_metadata\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w0g1NI39QUvL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 296,
          "status": "ok",
          "timestamp": 1717219525839,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "w0g1NI39QUvL",
        "outputId": "bb790f7d-0606-4af6-89d7-cdae4591f24c"
      },
      "outputs": [],
      "source": [
        "type(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KzeuP_VPRZDy",
      "metadata": {
        "id": "KzeuP_VPRZDy"
      },
      "source": [
        "Now let's use a chat PromptTemplate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "OHFg1PQa5REQ",
      "metadata": {
        "executionInfo": {
          "elapsed": 309,
          "status": "ok",
          "timestamp": 1717219540572,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "OHFg1PQa5REQ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant that helps extract entities from product descriptions.\"\n",
        "                \"You always respond in a json format.\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"Extract the following entities:\\n{entities}\\n from the item's description:\\n{description}.\"),\n",
        "    ]\n",
        ")\n",
        "chat_model = ChatVertexAI(model_name=\"gemini-1.5-pro-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "37jxEWCQT467",
      "metadata": {
        "executionInfo": {
          "elapsed": 1163,
          "status": "ok",
          "timestamp": 1717219700197,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "37jxEWCQT467"
      },
      "outputs": [],
      "source": [
        "chain = chat_template | chat_model | JsonOutputParser()\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WiUZP_UzUB_B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1717219704749,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "WiUZP_UzUB_B",
        "outputId": "6e58da73-4297-4c79-d30e-14ef436f8718"
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cp1p25CGOOsV",
      "metadata": {
        "id": "Cp1p25CGOOsV"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w_syWClaRc3A",
      "metadata": {
        "id": "w_syWClaRc3A"
      },
      "source": [
        "Let's use a pre-defined callback that memorizes amount of tokens consumed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "S9uAsguSOQU5",
      "metadata": {
        "executionInfo": {
          "elapsed": 1524,
          "status": "ok",
          "timestamp": 1717219764481,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "S9uAsguSOQU5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai.callbacks import VertexAICallbackHandler\n",
        "\n",
        "handler = VertexAICallbackHandler()\n",
        "\n",
        "config = {\n",
        "    'callbacks' : [handler]\n",
        "}\n",
        "result = chain.invoke({\"entities\": \"price, RAM\", \"description\": description}, \\\n",
        "                      config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TwrOqm_UO8T1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1717219764481,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "TwrOqm_UO8T1",
        "outputId": "b533502f-a9a2-4e5e-c393-9b4a3fe5eb36"
      },
      "outputs": [],
      "source": [
        "print(handler.prompt_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe46rBHjRTM2",
      "metadata": {
        "id": "fe46rBHjRTM2"
      },
      "source": [
        "## Use Codey model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQb324r7RmKo",
      "metadata": {
        "id": "KQb324r7RmKo"
      },
      "source": [
        "Codey models help you to write code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52c8198",
      "metadata": {},
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "from vertexai.vision_models import Image\n",
        "\n",
        "PROJECT_ID = \"ds-on-gsp\"\n",
        "REGION = \"us-central1\"\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "IMAGE_FILE = \"gs://ds-on-gsp-aiml-sa-bucket/chapter-18-data/func_generated_image.png\"\n",
        "image = Image.load_from_file(IMAGE_FILE)\n",
        "\n",
        "generative_multimodal_model = GenerativeModel(\"gemini-2.0-flash-exp\")\n",
        "response = generative_multimodal_model.generate_content([Part.from_text(\"What is shown in this image?\"), Part.from_uri(uri=IMAGE_FILE, mime_type=\"image/png\")])\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "deJUwscXOWe0",
      "metadata": {
        "executionInfo": {
          "elapsed": 340,
          "status": "ok",
          "timestamp": 1717219769254,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "deJUwscXOWe0"
      },
      "outputs": [],
      "source": [
        "# as of Jan 23, 2025, gemini-2.0 is available in certain regions, not including asia-northeast3\n",
        "codey_llm = VertexAI(model_name=\"gemini-2.0-flash-exp\", max_output_tokens=2048, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tfaEnhJIWfqp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2132,
          "status": "ok",
          "timestamp": 1717219780742,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "tfaEnhJIWfqp",
        "outputId": "f772fc97-613e-4d07-f987-007fde37f3a6"
      },
      "outputs": [],
      "source": [
        "print(codey_llm.invoke(\"Generate a python script to sort a list of integer numbers.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Oav-iDpXz4p",
      "metadata": {
        "id": "3Oav-iDpXz4p"
      },
      "source": [
        "## Try OSS models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OHvoiHLhRp2u",
      "metadata": {
        "id": "OHvoiHLhRp2u"
      },
      "source": [
        "You can also use open-source models with Vertex Model Garden. First, you need to deploy a model (e.g., LLama as described in a model card in Google Cloud consolde). After that, add your values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oJ3-XruNX08f",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1717220012671,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "oJ3-XruNX08f"
      },
      "outputs": [],
      "source": [
        "llama_endpoint_id = \"8520345401566429184\"\n",
        "projects = !gcloud config get project\n",
        "project = \"ds-on-gsp\"\n",
        "location = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91M4BagLX5wX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1501,
          "status": "ok",
          "timestamp": 1717220015792,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "91M4BagLX5wX",
        "outputId": "48b813a8-1bde-4a3a-ebb4-0017d993a7a8"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import VertexAIModelGarden\n",
        "\n",
        "llama_model = VertexAIModelGarden(\n",
        "    endpoint_id=llama_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        ")\n",
        "output = llama_model.invoke([\"How much is 2+2\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DFYy6XVSaq_O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1643,
          "status": "ok",
          "timestamp": 1717220043347,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "DFYy6XVSaq_O",
        "outputId": "f66c3a5c-31bc-44e1-c0c9-0cfe2e67e33d"
      },
      "outputs": [],
      "source": [
        "output = llama_model.invoke([\"Write a poem about LangChain and Google Cloud\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hrZXRVhmSo6w",
      "metadata": {
        "id": "hrZXRVhmSo6w"
      },
      "source": [
        "With Model Garden, you can use additional arguments that the model supports, but you need to provide them during model initialization (so that they're passed to the request):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y8zzGV9haxmq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 17264,
          "status": "ok",
          "timestamp": 1717220064989,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "Y8zzGV9haxmq",
        "outputId": "3a32a751-a16e-4a82-8226-881626faff38"
      },
      "outputs": [],
      "source": [
        "llama_model1 = VertexAIModelGarden(\n",
        "    endpoint_id=llama_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        "    allowed_model_args=[\"max_tokens\", \"top_k\"]\n",
        ")\n",
        "output = llama_model1.invoke([\"Write a poem about LangChain and Google Cloud\"], max_tokens=300)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PplTZfJ8Syhj",
      "metadata": {
        "id": "PplTZfJ8Syhj"
      },
      "source": [
        "Let's use another open source model, Falcon Instruct 40B deployed on Model Garden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "eyX9mrTQTb5U",
      "metadata": {
        "id": "eyX9mrTQTb5U"
      },
      "outputs": [],
      "source": [
        "falcon_endpoint_id = \"6259005125486968832\"\n",
        "project = \"ds-on-gsp\"\n",
        "location = \"asia-northeast3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gF-Z4GzFa6m_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 37341,
          "status": "ok",
          "timestamp": 1717220327887,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "gF-Z4GzFa6m_",
        "outputId": "69446b01-e4ce-4a50-9420-513110aa64f4"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import VertexAIModelGarden\n",
        "\n",
        "\n",
        "falcon_model = VertexAIModelGarden(\n",
        "    endpoint_id=falcon_endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        "    request_arg=\"generated_text\"\n",
        ")\n",
        "output = falcon_model.invoke([\"How old are you?\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3MRwP73TmpE",
      "metadata": {
        "id": "I3MRwP73TmpE"
      },
      "source": [
        "You can also use third-party models like Claude from Anthropic that don't require any deployment on Model Garden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "dPG2rWiETsH1",
      "metadata": {
        "executionInfo": {
          "elapsed": 341,
          "status": "ok",
          "timestamp": 1717220400675,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "dPG2rWiETsH1"
      },
      "outputs": [],
      "source": [
        "project = \"ds-on-gsp\"\n",
        "location = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ubUkThZbMO_",
      "metadata": {
        "executionInfo": {
          "elapsed": 2369,
          "status": "ok",
          "timestamp": 1717220404010,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "2ubUkThZbMO_"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
        "\n",
        "model = ChatAnthropicVertex(\n",
        "        project=project,\n",
        "        location=location,\n",
        "    )\n",
        "raw_system_message = (\n",
        "    \"You're a useful assistant that helps with math problems. Think step by step and provide reasoning for each step.\"\n",
        "    )\n",
        "question = (\n",
        "    \"Hello, how much is 2+2?\"\n",
        ")\n",
        "system_message = SystemMessage(content=raw_system_message)\n",
        "message = HumanMessage(content=question)\n",
        "response = model.invoke([system_message, message], model_name=\"claude-3-sonnet@20240229\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eRZ23KJ5T_oL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 314,
          "status": "ok",
          "timestamp": 1717220447643,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "eRZ23KJ5T_oL",
        "outputId": "329154ea-fdd5-4ed6-f6f8-d4724d004879"
      },
      "outputs": [],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PYuzrjulHuoP",
      "metadata": {
        "id": "PYuzrjulHuoP"
      },
      "source": [
        "# Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iqTOxxJPUawB",
      "metadata": {
        "id": "iqTOxxJPUawB"
      },
      "source": [
        "Let's look at example how we can improve our prompt and use LangChain interfaces for that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "3ZFpMSc1bQw2",
      "metadata": {
        "executionInfo": {
          "elapsed": 344,
          "status": "ok",
          "timestamp": 1717220454087,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "3ZFpMSc1bQw2"
      },
      "outputs": [],
      "source": [
        "instruction = (\n",
        "  \"---INSTRUCTION--- \\nYou are an intelligent assistant that helps marketers write great copy for campaigns on our website, \"\n",
        "  \"which sells premium ceiling fans to design-conscious customers. Please create campaign copy (a slogan, a tagline, a short \"\n",
        "  \"description, and three calls-to-action) based on keywords. Use the information from your context to choose the right products \"\n",
        "  \"to advertise. Follow the examples below to ensure that you follow company branding standards.\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "IkU-msKCb6FO",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1717220454783,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "IkU-msKCb6FO"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"keywords\": \"best fan for hot summer days, powerful, cozy, wood tone, enjoy cold drink\",\n",
        "        \"response\": (\n",
        "         \"Slogan:  Breeze 4000: Feel the Difference.\\n\"\n",
        "          \"Tagline: Design, Comfort, Performance – The Ultimate Summer Upgrade.\\n\"\n",
        "          \"Short Description:  Beat the heat in style with the Breeze 4000. Its sleek wood-tone design and \"\n",
        "          \"whisper-quiet operation create the perfect oasis for enjoying a cool drink on those hot summer days.\\n\"\n",
        "          \"Call to action: 1/ Experience the Breeze 4000 difference today.  (Emphasizes the unique qualities)\\n\"\n",
        "          \"2/ Upgrade your summer. Shop the Breeze 4000 now. (Creates a sense of urgency)\\n\"\n",
        "          \"3/ Find your perfect Breeze 4000 style. (Focus on design and personalization)\"\n",
        "        )\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "jGjofmKDIubR",
      "metadata": {
        "executionInfo": {
          "elapsed": 338,
          "status": "ok",
          "timestamp": 1717220456365,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "jGjofmKDIubR"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"---CONTEXT---\\n{context}\\n------KEYWORDS FOR CREATING COPY---\\n{keywords}\\n---EXAMPLES---\\n{examples}\"\n",
        "context = [\n",
        "  {\n",
        "    \"name\": \"Whirlwind BreezeMaster 3000\",\n",
        "    \"performanceRating\": \"high\",\n",
        "    \"outdoor\": True,\n",
        "    \"powerSource\": \"electric\",\n",
        "    \"price\": 249.99\n",
        "  }\n",
        "]\n",
        "keywords = \"best fan for dry heat, powerful, outdoor, porch, affordable\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZUOWwDKOJGG-",
      "metadata": {
        "executionInfo": {
          "elapsed": 344,
          "status": "ok",
          "timestamp": 1717220457982,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "ZUOWwDKOJGG-"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"keywords\", \"response\"], template=\"Example keywords:\\n{keywords}\\nExample response:\\n{response}\"\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=instruction,\n",
        "    suffix=\"---CONTEXT---\\n{context}\\n---KEYWORDS FOR CREATING COPY---\\n{keywords}\\n\",\n",
        "    input_variables=[\"context\", \"keywords\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zm98QI4pKbMh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1702,
          "status": "ok",
          "timestamp": 1717220476874,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -120
        },
        "id": "zm98QI4pKbMh",
        "outputId": "07507881-0468-4495-93d5-fb07e8d63cfe"
      },
      "outputs": [],
      "source": [
        "llm = VertexAI(model_name=\"gemini-2.0-flash-exp\", location=\"us-central1\")\n",
        "\n",
        "respose = (prompt | llm).invoke({\"context\": context, \"keywords\": keywords})\n",
        "print(respose)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Chapter 2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
